{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5 -- Human Detection \n",
    "This tutorial will show you how to detect human with the robot. We will use multi-thread to capture the color images and depth images. Then the ssd mobilenet v2 nerual network structure is used for human detection. To enable real-time performance, the tensorRT deep learning framework is used. More information can be found in the nvidia's jetbot project website: https://github.com/NVIDIA-AI-IOT/jetbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 load the pre-trained object detection model\n",
    "\n",
    "Before running the program, we need to download the pre-trained detection model and put it in the current folder. The model can be found in the following link:https://drive.google.com/file/d/1KjlDMRD8uhgQmQK-nC2CZGHFTbq4qQQH/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "from tensorrt_model import TRTModel\n",
    "from ssd_tensorrt import load_plugins, parse_boxes,TRT_INPUT_NAME, TRT_OUTPUT_NAME\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import ctypes\n",
    "    \n",
    "mean = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "stdev = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "def bgr8_to_ssd_input(camera_value):\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1)).astype(np.float32)\n",
    "    x -= mean[:, None, None]\n",
    "    x /= stdev[:, None, None]\n",
    "    return x[None, ...]\n",
    "\n",
    "class ObjectDetector(object):\n",
    "    \n",
    "    def __init__(self, engine_path, preprocess_fn=bgr8_to_ssd_input):\n",
    "        logger = trt.Logger()\n",
    "        trt.init_libnvinfer_plugins(logger, '')\n",
    "        load_plugins()\n",
    "        self.trt_model = TRTModel(engine_path, input_names=[TRT_INPUT_NAME],output_names=[TRT_OUTPUT_NAME, TRT_OUTPUT_NAME + '_1'])\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        \n",
    "    def execute(self, *inputs):\n",
    "        trt_outputs = self.trt_model(self.preprocess_fn(*inputs))\n",
    "        return parse_boxes(trt_outputs)\n",
    "    \n",
    "    def __call__(self, *inputs):\n",
    "        return self.execute(*inputs)\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Initialize the camera instance for the Intel realsense sensor D435i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use traitlets and widgets to display the image in Jupyter Notebook\n",
    "import traitlets\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "#use opencv to covert the depth image to RGB image for displaying purpose\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#using realsense to capture the color and depth image\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "#multi-threading is used to capture the image in real time performance\n",
    "import threading\n",
    "\n",
    "class Camera(SingletonConfigurable):\n",
    "    \n",
    "    #this changing of these two values will be captured by traitlets\n",
    "    color_value = traitlets.Any()\n",
    "    depth_value = traitlets.Any()\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "        \n",
    "        #configure the color and depth sensor\n",
    "        self.pipeline = rs.pipeline()\n",
    "        self.configuration = rs.config()  \n",
    "        \n",
    "        #set resolution for the color camera\n",
    "        self.color_width = 640\n",
    "        self.color_height = 480\n",
    "        self.color_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.color, self.color_width, self.color_height, rs.format.bgr8, self.color_fps)\n",
    "\n",
    "        #set resolution for the depth camera\n",
    "        self.depth_width = 640\n",
    "        self.depth_height = 480\n",
    "        self.depth_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.depth, self.depth_width, self.depth_height, rs.format.z16, self.depth_fps)\n",
    "\n",
    "        self.stop_flag = False\n",
    "        \n",
    "        #start capture the first depth and color image\n",
    "        self.pipeline.start(self.configuration)\n",
    "        self.pipeline_started = True\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "\n",
    "        color_frame = frames.get_color_frame()   \n",
    "        image = np.asanyarray(color_frame.get_data())\n",
    "        self.color_value = image\n",
    "\n",
    "        depth_frame = frames.get_depth_frame()           \n",
    "        self.depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(self.depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        self.depth_value = depth_colormap   \n",
    "        \n",
    "        self.warning_flag = 0\n",
    "        self.min_distance = 0\n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while(self.stop_flag==False):\n",
    "            frames = self.pipeline.wait_for_frames()\n",
    "            color_frame = frames.get_color_frame()\n",
    "            image = np.asanyarray(color_frame.get_data())\n",
    "            self.color_value = image\n",
    "\n",
    "            depth_frame = frames.get_depth_frame()              \n",
    "            self.depth_image = np.asanyarray(depth_frame.get_data())\n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(self.depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            self.depth_value = depth_colormap\n",
    "            \n",
    "            depths = self.depth_image[self.depth_image!=0]\n",
    "            if depths.size == 0:\n",
    "                self.min_distance = 0\n",
    "            else:\n",
    "                self.min_distance = depths.min()\n",
    "            self.warning_flag = (self.min_distance < 200)\n",
    "    \n",
    "    def start(self):\n",
    "        if not hasattr(self, 'thread') or not self.thread.isAlive():\n",
    "            self.thread = threading.Thread(target=self._capture_frames)\n",
    "            self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_flag = True\n",
    "        if hasattr(self, 'thread'):\n",
    "            self.thread.join()        \n",
    "        if self.pipeline_started:\n",
    "            self.pipeline.stop()\n",
    "\n",
    "def bgr8_to_jpeg(value,quality=75):\n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera.instance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.start() # start capturing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Perform object detection and display the results on widgets\n",
    "Human is labeled is 1 in the pretrained model. A full list of the detection class indices can be found in the following link https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt\n",
    "\n",
    "The following program will ask the robot to move forward if a human is detected in the captured color image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923d2b8dff034f0a9b85f04f28be0601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=40, description='Obstacle', max=4000, min=40)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mochris import RangeSensors\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "obstacle_slider = widgets.IntSlider(description='Obstacle', min=40, max=4000, orientation='horizontal')\n",
    "\n",
    "range_sensors = RangeSensors(front=True)\n",
    "\n",
    "def update(change):\n",
    "    front_range = change['new']    \n",
    "    obstacle_slider.value = front_range\n",
    "\n",
    "range_sensors.observe(update, names='front_value')\n",
    "\n",
    "display(obstacle_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fca9de6c72b488fa2e656d04d76a480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'),)), IntText(value=27,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from random import random\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "# Creates a widgit to display the image from camera on\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "\n",
    "# Creates the label widget to select tracked object\n",
    "label_widget = widgets.IntText(value=27, description='tracked label')\n",
    "\n",
    "# Creates a label widget to show the current distance from tracked object\n",
    "label_distance = widgets.IntText(value=1, description='distance')\n",
    "\n",
    "# Creates a label widget to show the minimum distance from tracked object\n",
    "label_min_distance = widgets.IntText(value=0, description='minimum distance')\n",
    "\n",
    "# Creates the drop down menu to select the different behaviors.\n",
    "# Default is love.\n",
    "mode_widget = widgets.Dropdown(\n",
    "    options=['aggressive', 'fear', 'love', 'curious'],\n",
    "    value='love',\n",
    "    description='Mode: ',\n",
    "    disabled=False,\n",
    ")\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget,]),\n",
    "    label_widget,\n",
    "    label_distance,\n",
    "    obstacle_slider,\n",
    "    mode_widget\n",
    "]))\n",
    "\n",
    "import time\n",
    "from RobotClass import Robot\n",
    "\n",
    "#initialize the Robot class\n",
    "robot = Robot()\n",
    "\n",
    "# Defines the current state the robot is in for the love behavior. \n",
    "love_flag = 0\n",
    "\n",
    "# Defines when the robot should be stopped. \n",
    "stop_flag = 0\n",
    "\n",
    "# Defines the direction the robot should turn.\n",
    "# 0 = right\n",
    "# 1 = left\n",
    "left_flag = 1\n",
    "\n",
    "# The love behavior logic. When love is selected, this function is called\n",
    "# and the robot performs a left,right motion going forward.\n",
    "def love():\n",
    "    global love_flag\n",
    "    if love_flag == 0:\n",
    "        robot.left(0.5)\n",
    "    elif love_flag == 1:\n",
    "        robot.forward(0.3)\n",
    "    elif love_flag == 2:\n",
    "        robot.right(0.5)\n",
    "    elif love_flag == 3:\n",
    "        robot.forward(0.3)\n",
    "    love_flag = (love_flag + 1) % 4\n",
    "\n",
    "curious_flag = 0\n",
    "\n",
    "# The curious behavior logic. \n",
    "# param direction: The direction the robot should move, value should be 'left' or 'right'\n",
    "def curious(direction):\n",
    "    global curious_flag\n",
    "    if curious_flag <= 10:\n",
    "        if direction == 'left':\n",
    "            robot.forward_left(0.5)\n",
    "        elif direction == 'right':\n",
    "            robot.forward_right(0.5)\n",
    "        else:\n",
    "            robot.forward(0.5)\n",
    "    else:\n",
    "        robot.backward(0.3)\n",
    "    curious_flag = (curious_flag + 1) % 20\n",
    "\n",
    "def processing(change):\n",
    "    image = change['new']\n",
    "    depth = camera.depth_image\n",
    "    global stop_flag\n",
    "    global left_flag\n",
    "    \n",
    "    # The number of objects on the left hand side of the screen     \n",
    "    left_count = 0\n",
    "    \n",
    "    # The number of objects in the middle of the screen     \n",
    "    center_count = 0\n",
    "    \n",
    "    # The number of objects on the right hand side of the screen    \n",
    "    right_count = 0\n",
    "    size_largest = 0\n",
    "     \n",
    "    imgsized= cv2.resize(image,(300,300))\n",
    "    \n",
    "    # compute all detected objects\n",
    "    detections = model(imgsized)\n",
    "    \n",
    "    # Gets all the detected object that match the object we are trying to detect.      \n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    for det in matching_detections:\n",
    "        bbox = det['bbox']\n",
    "        \n",
    "        x1 = int(width * bbox[0])\n",
    "        x2 = int(width * bbox[2])\n",
    "        y1 = int(width * bbox[1])\n",
    "        y2 = int(width * bbox[3])\n",
    "        depths = depth[y1:y2, x1:x2]\n",
    "        depths = depths[depths != 0]\n",
    "        distance = np.nanmean(depths)\n",
    "        \n",
    "        # Draws a bounding box around objects that are not the tracked one.         \n",
    "        if det['confidence'] < 0:\n",
    "            cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "            continue\n",
    "        else:\n",
    "            # Draw an image in red around the detected image             \n",
    "            cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 0, 255), 2)\n",
    "      \n",
    "        # Counts the number of objects on the left, right and center of the image.         \n",
    "        if (bbox[0]+bbox[2])/2 < 2/5.0:\n",
    "            left_count = left_count + 1\n",
    "        elif (bbox[0]+bbox[2])/2 > 3/5.0:\n",
    "            right_count = right_count + 1\n",
    "        else:\n",
    "            center_count = center_count + 1\n",
    "        label_distance.value = distance\n",
    "        size = (bbox[2] - bbox[0]) * (bbox[3] - bbox[2])\n",
    "        if size > size_largest:\n",
    "            size_largest = size\n",
    "        \n",
    "    label_min_distance.value = camera.min_distance\n",
    "    \n",
    "    # Wander behavior     \n",
    "    if love_flag:\n",
    "        love()\n",
    "        if len(matching_detections) == 0:\n",
    "            stop_flag = 3\n",
    "    elif len(matching_detections) == 0:\n",
    "        if random() < 0.5:\n",
    "            if stop_flag:\n",
    "                stop_flag = max(stop_flag - 1, 0)\n",
    "                robot.stop()\n",
    "            else:\n",
    "                if left_flag:\n",
    "                    robot.left(0.5)\n",
    "                else:\n",
    "                    robot.right(0.5)\n",
    "        else:\n",
    "            robot.stop()\n",
    "    else:\n",
    "        if left_count > right_count and left_count > center_count:\n",
    "            left_flag = 1\n",
    "        if right_count > left_count and right_count > center_count:\n",
    "            left_flag = 0\n",
    "        stop_flag = 0\n",
    "        \n",
    "        # Handles the fear behavior         \n",
    "        if mode_widget.value == 'fear':\n",
    "            if distance < 1800:\n",
    "                robot.backward(max((3000-distance)/3000*2, 0.3))\n",
    "            else:\n",
    "                stop_flag = 3\n",
    "                robot.stop()\n",
    "         \n",
    "        # Handles the aggression behavior         \n",
    "        elif mode_widget.value == 'aggressive':\n",
    "            if distance < 1200:\n",
    "                robot.stop()\n",
    "            elif left_count > right_count and left_count > center_count:\n",
    "                left_flag = 1\n",
    "                robot.forward_left(max((5000-distance)/2500*2+0.3, 0.3))\n",
    "            elif right_count > left_count and right_count > center_count:\n",
    "                left_flag = 0\n",
    "                robot.forward_right(max((5000-distance)/2500*2+0.3, 0.3))\n",
    "            else:\n",
    "                robot.forward(max((2000-distance)/2000*2, 0.3))\n",
    "                \n",
    "        # Handles the love behavior                 \n",
    "        elif mode_widget.value == 'love':\n",
    "            if distance < 900:\n",
    "                robot.stop()\n",
    "            elif left_count > right_count:\n",
    "                stop_flag = 3\n",
    "                robot.left(0.5)\n",
    "            elif right_count > left_count:\n",
    "                stop_flag = 3\n",
    "                robot.right(0.5)\n",
    "            else:\n",
    "                love()\n",
    "                \n",
    "        # Handles the curious behavior                 \n",
    "        elif mode_widget.value == 'curious':\n",
    "            if distance < 900:\n",
    "                robot.stop()\n",
    "            elif left_count > right_count and left_count > center_count:\n",
    "                stop_flag = 3\n",
    "                robot.left(0.5)\n",
    "            elif right_count > left_count and right_count > center_count:\n",
    "                stop_flag = 3\n",
    "                robot.right(0.5)\n",
    "            else:\n",
    "                curious('center')\n",
    "    \n",
    "\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "processing({'new': camera.color_value})\n",
    "camera.observe(processing, names=['color_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code can be used to stop the capturing of the image and the moving of the robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "### 1. Please try to calculate the distance between the detected human and the robot using the depth image. (Note: You can refer to tutorial 2 to get the depth information for a specific point)\n",
    "### 2. Please try to add collision avoidance function into this program to protect the robot. (Note: You can refer to tutorial 4 for the collision avoidance)\n",
    "### 3. Think about how to control the robot to move towards the detected human"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
